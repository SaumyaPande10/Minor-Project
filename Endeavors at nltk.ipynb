{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# My endeavors to learn nltk as part of the minor project I will be working on \n",
    "### corpora - body of text. eg: medical journals, presidential speeches etc\n",
    "### lexicon - words and their meanings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLTK works with different languages as well other than english!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_text = \"Hello Mr. Smith. It is a good day. I am pleased to meet you. I hope happiness comes your way.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello Mr. Smith.', 'It is a good day.', 'I am pleased to meet you.', 'I hope happiness comes your way.']\n"
     ]
    }
   ],
   "source": [
    "print(sent_tokenize(example_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'Mr.', 'Smith', '.', 'It', 'is', 'a', 'good', 'day', '.', 'I', 'am', 'pleased', 'to', 'meet', 'you', '.', 'I', 'hope', 'happiness', 'comes', 'your', 'way', '.']\n"
     ]
    }
   ],
   "source": [
    "print(word_tokenize(example_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello\n",
      "Mr.\n",
      "Smith\n",
      ".\n",
      "It\n",
      "is\n",
      "a\n",
      "good\n",
      "day\n",
      ".\n",
      "I\n",
      "am\n",
      "pleased\n",
      "to\n",
      "meet\n",
      "you\n",
      ".\n",
      "I\n",
      "hope\n",
      "happiness\n",
      "comes\n",
      "your\n",
      "way\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "for i in word_tokenize(example_text):\n",
    "    print(i)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop words : The words that we dont care about and dont need in our analysis like a, the, and etc, and also words that create general confusion like sarcastic words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_sentence = \"This is an example sentence for to check how does the stopwords work. Mind you nltk already has a library of english stopwords. You need not provide one.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words(\"english\"))\n",
    "# sets english as the defalut list of stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ve', 'does', 'very', 'further', 'will', 'after', 'had', 'only', 'm', \"aren't\", 'this', 'in', 'so', 'few', 'but', 'during', 'she', 'aren', 'has', 'with', 'about', 'they', \"don't\", 'here', 'until', 'all', 'on', 'for', 'did', \"weren't\", 'from', 'which', 'before', 'any', 's', 'off', 'too', \"won't\", 'by', 'itself', 'it', 'am', 'how', 'i', 'both', 'isn', 'them', \"shouldn't\", 'nor', 'myself', 'shouldn', 'these', 'most', 'mightn', 'we', 'the', 'her', 'have', \"you've\", 'into', 'as', 'mustn', \"that'll\", 'ours', \"hadn't\", \"hasn't\", 'over', 'd', 'weren', 'at', 'than', 'me', 'up', 'above', \"didn't\", 'ourselves', 'shan', 'again', 'been', 'below', 'can', 'him', 'are', 'to', 'your', 'there', 'when', 'or', 'more', \"mightn't\", 'herself', 'was', \"wouldn't\", 'were', 'those', 'own', 'is', 'between', 'yourself', 'just', 'who', 'what', 'and', 'hasn', \"you're\", 'while', 'against', 'you', 'themselves', 'being', 'then', 'why', \"she's\", 'an', 'if', \"needn't\", 'doing', \"wasn't\", 'of', 'doesn', 'wouldn', 'not', 'through', 'ain', 'y', 'himself', \"couldn't\", 'won', 'out', 'theirs', 'down', 'our', 'my', \"doesn't\", \"you'll\", 're', 'their', 'didn', 'll', 'its', 'he', \"isn't\", 'yourselves', 'a', 'should', 'under', \"haven't\", \"shan't\", 'hadn', 'wasn', 'his', 'where', 'each', 'don', 'that', \"you'd\", 'no', 'some', 'yours', 'once', 'same', 'ma', 'needn', \"should've\", 'be', 'whom', 'o', 'having', \"it's\", \"mustn't\", 't', 'because', 'do', 'haven', 'such', 'hers', 'now', 'couldn', 'other'}\n"
     ]
    }
   ],
   "source": [
    "print(stop_words)\n",
    "# These are the set of predefined stopwords in the library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = word_tokenize(example_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'example', 'sentence', 'check', 'stopwords', 'work', '.', 'Mind', 'nltk', 'already', 'library', 'english', 'stopwords', '.', 'You', 'need', 'provide', 'one', '.']\n"
     ]
    }
   ],
   "source": [
    "filtered_sentence = []\n",
    "for w in words:\n",
    "    if w not in stop_words:\n",
    "        filtered_sentence.append(w)\n",
    "print(filtered_sentence)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# another short way or oneliner to do the same thing\n",
    "filtered_sentence = [w for w in words if not w in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'example', 'sentence', 'check', 'stopwords', 'work', '.', 'Mind', 'nltk', 'already', 'library', 'english', 'stopwords', '.', 'You', 'need', 'provide', 'one', '.']\n"
     ]
    }
   ],
   "source": [
    "print(filtered_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming : ride and riding are same\n",
    "### alternative to it is wordnet and sinset which is more frequently used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python\n",
      "python\n",
      "pythonli\n",
      "pythonista\n",
      "python\n"
     ]
    }
   ],
   "source": [
    "ps = PorterStemmer()\n",
    "example_words = [\"python\",\"pythoner\",\"pythonly\",\"pythonista\",\"pythoning\"]\n",
    "for w in example_words:\n",
    "    print(ps.stem(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aim\n",
      "high\n",
      "is\n",
      "our\n",
      "duti\n",
      ",\n",
      "dream\n",
      "is\n",
      "also\n",
      "our\n",
      "duti\n",
      ".\n",
      "endeavor\n",
      "to\n",
      "learn\n",
      "must\n",
      "not\n",
      "stop\n",
      ".\n",
      "desir\n",
      "to\n",
      "grow\n",
      "must\n",
      "grow\n",
      "on\n",
      ".\n",
      "commun\n",
      "growth\n",
      "becom\n",
      "import\n",
      "for\n",
      "the\n",
      "develop\n",
      "of\n",
      "the\n",
      "nation\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "new_text = \"Aiming high is our duty, dreaming is also our duty. Endeavors to learn must not stop. Desire to grow must grow on. Community growth becomes important for the development of the nation.\"\n",
    "words = word_tokenize(new_text)\n",
    "\n",
    "for w in words:\n",
    "    print(ps.stem(w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Speech Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import state_union\n",
    "# state union addresses by various presidents over the past 60 yrs\n",
    "from nltk.tokenize import PunktSentenceTokenizer\n",
    "# It is an unsupervised ML sentence tokenizer, though it is pretrained we can always retrain it\n",
    "sample_test = state_union.raw(\"2006\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Sukanya', 'NNP'), (',', ','), ('Rajib', 'NNP'), ('Naba', 'NNP'), ('good', 'JJ'), ('friends', 'NNS'), ('.', '.')]\n",
      "[('Sukanya', 'NNP'), ('getting', 'VBG'), ('married', 'VBD'), ('next', 'JJ'), ('yearMarriage', 'NN'), ('big', 'JJ'), ('step', 'NN'), ('one', 'CD'), ('â€™', 'NNP'), ('life', 'NN'), ('.', '.')]\n",
      "[('It', 'PRP'), ('exciting', 'VBG'), ('frightening', 'VBG'), ('.', '.')]\n",
      "[('But', 'CC'), ('friendship', 'NN'), ('sacred', 'VBD'), ('bond', 'NN'), ('people', 'NNS'), ('.', '.')]\n",
      "[('It', 'PRP'), ('special', 'JJ'), ('kind', 'NN'), ('love', 'VB'), ('us', 'PRP'), ('.', '.')]\n",
      "[('Many', 'JJ'), ('must', 'MD'), ('tried', 'VB'), ('searching', 'VBG'), ('friend', 'NN'), ('never', 'RB'), ('found', 'VBD'), ('right', 'JJ'), ('one', 'CD'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Dummy text \n",
    "txt = \"Sukanya, Rajib and Naba are my good friends. Sukanya is getting married next yearMarriage is a big step in oneâ€™s life. It is both exciting and frightening.   But friendship is a sacred bond between people.  It is a special kind of love between us.  Many of you must have tried searching for a friend   but never found the right one.\"\n",
    "    \n",
    "  \n",
    "# sent_tokenize is one of instances of  \n",
    "# PunktSentenceTokenizer from the nltk.tokenize.punkt module \n",
    "  \n",
    "tokenized = sent_tokenize(txt) \n",
    "for i in tokenized: \n",
    "      \n",
    "    # Word tokenizers is used to find the words  \n",
    "    # and punctuation in a string \n",
    "    wordsList = nltk.word_tokenize(i) \n",
    "  \n",
    "    # removing stop words from wordList \n",
    "    wordsList = [w for w in wordsList if not w in stop_words]  \n",
    "  \n",
    "    #  Using a Tagger. Which is part-of-speech  \n",
    "    # tagger or POS-tagger.  \n",
    "    tagged = nltk.pos_tag(wordsList) \n",
    "  \n",
    "    print(tagged) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chunking\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
